from datetime import datetime
import cts_parser
import requests
import os.path

#result from parser_page(url)
target_result = {
    "url": u'http://news.cts.com.tw/cts/society/201607/201607071771440.html#.V34Qk7h942w',
    "source_press": u'http://www.cts.com.tw',
    "title": u'\xe9\xa8\x99\xe6\x95\x99\xe6\x9c\x83\xe7\xa6\x8f\xe5\x82\xb3\xe5\xb8\xab \xe6\xaf\x92\xe7\x8a\xaf\xe5\x81\xb7\xe8\xbb\x8a\xe8\xa1\x8c\xe7\xab\x8a',
    "post_time": datetime(2016, 7, 7, 12, 35),
    "journalist": u'\xe7\xb6\x9c\xe5\x90\x88\xe5 \xb1\xe5\xb0\x8e / \xe5\x98\x89\xe7\xbe\xa9\xe5\xb8\x82',
    "content": u'\xe5\x98\x89\xe7\xbe\xa9\xe4\xb8\x80\xe5\x90\x8d48\xe6\xad\xb2\xe7\x9a\x84\xe6\x9e\x97\xe5\xa7\x93\xe6\xaf\x92\xe8\xb2\xa9\xef\xbc\x8c\xe8\xa2\xab\xe5\xa4\xa9\xe4\xb8\xbb\xe6\x95\x99\xe9\xb9\xbf\xe8\x8d\x89\xe6\x95\x99\xe6\x9c\x83\xe7\x9a\x84\xe4\xb8\x80\xe5\x90\x8d\xe7\x86\xb1\xe5\xbf\x83\xe7\x9a\x84\xe7\xa6\x8f\xe5\x82\xb3\xe5\xb8\xab\xe6\x94\xb6\xe7\x95\x99\xe5\x9c\xa8\xe6\x95\x99\xe6\x9c\x83\xe8\xa3\xa1\xe6\x88\x92\xe6\xaf\x92\xef\xbc\x8c\xe6\xb2\x92\xe6\x83\xb3\xe5\x88\xb0\xe4\xbb\x96\xe9\x82\x84\xe6\x98\xaf\xe6\xaf\x92\xe7\x99\xae\xe9\x9b\xa3\xe8\x80\x90\xef\xbc\x8c\xe7\xab\x9f\xe7\x84\xb6\xe9\xa8\x8e\xe8\x91\x97\xe7\xa6\x8f\xe5\x82\xb3\xe5\xb8\xab\xe7\x9a\x84\xe6\xa9\x9f\xe8\xbb\x8a\xef\xbc\x8c\xe5\x8e\xbb\xe5\x81\xb7\xe5\xbb\x9f\xe5\xae\x87\xe8\xa3\xa1\xe7\xa5\x9e\xe6\x98\x8e\xe7\x9a\x84\xe9\x87\x91\xe7\x89\x8c\xe5\x86\x8d\xe8\xae\x8a\xe8\xb3\xa3\xe8\xb3\xbc\xe6\xaf\x92\xef\xbc\x8c\xe8\xad\xa6\xe6\x96\xb9\xe6\xa0\xb9\xe6\x93\x9a\xe6\xa9\x9f\xe8\xbb\x8a\xe8\xbb\x8a\xe7\x89\x8c\xe6\x89\xbe\xe5\x88\xb0\xe6\x95\x99\xe6\x9c\x83\xe4\xbe\x86\xef\xbc\x8c\xe7\xa6\x8f\xe5\x82\xb3\xe5\xb8\xab\xe5\xbe\x97\xe7\x9f\xa5\xe6\x9e\x97\xe5\xa7\x93\xe6\xaf\x92\xe8\xb2\xa9\xe9\x9d\x9e\xe4\xbd\x86\xe6\xb2\x92\xe6\x9c\x89\xe7\x9c\x9f\xe5\xbf\x83\xe6\x88\x92\xe6\xaf\x92\xe9\x82\x84\xe7\x95\xb6\xe5\x81\xb7\xe7\xab\x8a\xef\xbc\x8c\xe8\xae\x93\xe4\xbb\x96\xe6\x84\x9f\xe5\x88\xb0\xe9\x9d\x9e\xe5\xb8\xb8\xe7\x9a\x84\xe7\x97\x9b\xe5\xbf\x83\xef\xbc\x8c\xe4\xb8\x8d\xe9\x81\x8e\xe5\xa5\xbd\xe5\xbf\x83\xe7\x9a\x84\xe7\xa6\x8f\xe5\x82\xb3\xe5\xb8\xab\xe8\xaa\xaa\xe9\x82\x84\xe6\x98\xaf\xe9\xa1\x98\xe6\x84\x8f\xe5\xb9\xab\xe5\x8a\xa9\xe4\xbb\x96\xe3\x80\x82',
    "compare": None,
    "keyword": [u'\xe5\x81\xb7\xe7\xab\x8a', u'\xe5\xa4\xa9\xe4\xb8\xbb\xe6\x95\x99', u'\xe6\xaf\x92\xe8\xb2\xa9', u'\xe5\x98\x89\xe7\xbe\xa9'],
    "fb_like": 0,
    "fb_share": 0,
    "category": [u'\xe7\xa4\xbe\xe6\x9c\x83'],
    "comment": [],
}

#result from get_category_urls(url)
file_path = os.path.dirname(os.path.abspath(__file__))
file = open(file_path + '/tests/get_category_urls/fake_data', 'r')
content = file.read()
file.close()
    
number = content.count(',')
detail_urls = []
for i in range(number):
    index = content.index(',')
    detail_urls.append(content[0:index])
    content = content[index+2:]


#test parser_page(url)
url = "http://news.cts.com.tw/cts/society/201607/201607071771440.html#.V34Qk7h942w"
result = cts_parser.parser_page(url)
for key in target_result.keys():
    if target_result[key] != result[key]:
        print "Fail"
        break
else:
    print "Succeed"


#test get_category_urls(url)
url = "http://news.cts.com.tw/weather/index.html"
result = cts_parser.get_category_urls(url)
if len(set(result).intersection(set(detail_urls))) == 200:
    print "Succeed"
else:
    print "Fail"
    print len(set(result).intersection(set(detail_urls)))